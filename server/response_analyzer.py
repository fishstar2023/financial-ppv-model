"""
å›ç­”å¤šæ¨£æ€§åˆ†æå·¥å…·
ç”¨æ–¼è§€æ¸¬ AI ç”Ÿæˆå›ç­”çš„é‡è¤‡æ¨¡å¼ï¼Œæ‰¾å‡ºéœ€è¦æ”¹é€²çš„åœ°æ–¹
"""
import json
import re
from typing import List, Dict, Any
from collections import Counter
from pathlib import Path


def load_all_responses() -> List[Dict[str, Any]]:
    """å¾ JSON è¼‰å…¥æ‰€æœ‰è¨ªè«‡å›ç­”"""
    db_file = Path("server/vietnam_personas.json")
    if not db_file.exists():
        return []

    with open(db_file, 'r', encoding='utf-8') as f:
        personas = json.load(f)

    responses = []
    for persona in personas:
        for record in persona.get('interviewHistory', []):
            responses.append({
                'persona_id': persona.get('id'),
                'persona_name': persona.get('lastName'),
                'question': record.get('question', ''),
                'answer': record.get('answer', ''),
                'timestamp': record.get('timestamp', '')
            })
    return responses


def analyze_opening_patterns(responses: List[Dict]) -> Dict[str, int]:
    """åˆ†æå›ç­”é–‹é ­çš„æ¨¡å¼"""
    # åˆ†é¡ï¼šğŸš« = å·²ç¦æ­¢, âš ï¸ = æ‡‰ç›£æ§, âœ… = å¥½çš„é–‹é ­
    patterns = {
        # ğŸš« å·²ç¦æ­¢çš„é«˜é »é–‹é ­
        'å…¶å¯¦': 0,      # ğŸš« BANNED - 37.7%
        'å—¯': 0,        # ğŸš« BANNED - 19.2%
        'å“¦': 0,        # ğŸš« BANNED - 12.1%
        'æ¬¸': 0,        # ğŸš« BANNED - 11.7%
        'é‚£æ™‚å€™': 0,    # ğŸš« BANNED
        # âš ï¸ æ‡‰ç›£æ§çš„é–‹é ­
        'ç•¶æ™‚': 0,
        'è¨˜å¾—': 0,
        'èªªå¯¦è©±': 0,
        'å¤§æ¦‚': 0,
        'æ€éº¼èªª': 0,
        'æœ¬ä¾†': 0,
        'è€å¯¦èªª': 0,
        'å°±æ˜¯': 0,
        'æ˜¯æˆ‘': 0,
        'æˆ‘ç¬¬ä¸€æ¬¡': 0,
        # âœ… å¥½çš„å¤šæ¨£åŒ–é–‹é ­
        'èªªåˆ°é€™å€‹': 0,
        'å”‰': 0,
        'ä½ çŸ¥é“å—': 0,
        'è¬›ä¸€å€‹': 0,
        'å¦ç™½èªª': 0,
        'è®“æˆ‘æƒ³': 0,
        'å¥½ï¼Œ': 0,
        'ä¸çŸ¥é“': 0,
        'é€™è¦å¾': 0,
    }

    for resp in responses:
        answer = resp['answer'].strip()
        if not answer:
            continue
        # å–å‰ 20 å€‹å­—
        opening = answer[:20]
        for pattern in patterns:
            if pattern in opening:
                patterns[pattern] += 1

    return dict(sorted(patterns.items(), key=lambda x: x[1], reverse=True))


def analyze_ending_patterns(responses: List[Dict]) -> Dict[str, int]:
    """åˆ†æå›ç­”çµå°¾çš„æ¨¡å¼"""
    patterns = {
        'å€¼å¾—': 0,
        'å¾ˆå€¼å¾—': 0,
        'ç¸½ä¹‹': 0,
        'å­¸åˆ°': 0,
        'ç¶“é©—': 0,
        'å»ºè­°': 0,
        'ä¸‹æ¬¡': 0,
        'å°±é€™æ¨£': 0,
        'å§': 0,
        'å•¦': 0,
        'å“ˆå“ˆ': 0,
        'ä¸ç¢ºå®š': 0,
        'çœ‹çœ‹': 0,
        'å†èªª': 0,
    }

    for resp in responses:
        answer = resp['answer'].strip()
        if not answer:
            continue
        # å–å¾Œ 30 å€‹å­—
        ending = answer[-30:] if len(answer) > 30 else answer
        for pattern in patterns:
            if pattern in ending:
                patterns[pattern] += 1

    return dict(sorted(patterns.items(), key=lambda x: x[1], reverse=True))


def analyze_sentiment_words(responses: List[Dict]) -> Dict[str, int]:
    """åˆ†ææƒ…æ„Ÿè©å½™çš„ä½¿ç”¨"""
    positive_words = ['å¥½', 'æ£’', 'æ–¹ä¾¿', 'å¿«é€Ÿ', 'æ¨è–¦', 'æ»¿æ„', 'ä¸éŒ¯', 'å–œæ­¡', 'å®‰å¿ƒ', 'æ”¾å¿ƒ']
    negative_words = ['ç…©', 'æ°£', 'æ…¢', 'è¤‡é›œ', 'éº»ç…©', 'é›£', 'è²´', 'å·®', 'çˆ›', 'è¨å­', 'ç”Ÿæ°£', 'å¤±æœ›']
    neutral_words = ['æ™®é€š', 'ä¸€èˆ¬', 'é‚„å¥½', 'å·®ä¸å¤š', 'éƒ½å¯ä»¥']

    counts = {
        'positive': Counter(),
        'negative': Counter(),
        'neutral': Counter()
    }

    for resp in responses:
        answer = resp['answer']
        for word in positive_words:
            if word in answer:
                counts['positive'][word] += answer.count(word)
        for word in negative_words:
            if word in answer:
                counts['negative'][word] += answer.count(word)
        for word in neutral_words:
            if word in answer:
                counts['neutral'][word] += answer.count(word)

    return {
        'positive': dict(counts['positive'].most_common(10)),
        'negative': dict(counts['negative'].most_common(10)),
        'neutral': dict(counts['neutral'].most_common(10)),
        'total_positive': sum(counts['positive'].values()),
        'total_negative': sum(counts['negative'].values()),
        'total_neutral': sum(counts['neutral'].values()),
    }


def analyze_structure_patterns(responses: List[Dict]) -> Dict[str, Any]:
    """åˆ†æå›ç­”çµæ§‹æ¨¡å¼"""
    # æª¢æ¸¬å¸¸è¦‹çµæ§‹
    patterns = {
        'chronological': 0,  # æ™‚é–“é †åºï¼ˆç¬¬ä¸€æ¬¡...ç„¶å¾Œ...å¾Œä¾†...ï¼‰
        'comparison': 0,     # æ¯”è¼ƒå‹ï¼ˆè·Ÿ...æ¯”èµ·ä¾†ï¼‰
        'problem_solution': 0,  # å•é¡Œè§£æ±ºå‹ï¼ˆé‡åˆ°...ç„¶å¾Œè§£æ±ºï¼‰
        'list_style': 0,     # åˆ—èˆ‰å‹ï¼ˆé¦–å…ˆ...å…¶æ¬¡...ï¼‰
    }

    chronological_markers = ['ç¬¬ä¸€æ¬¡', 'å¾Œä¾†', 'ä¹‹å¾Œ', 'ç„¶å¾Œ', 'æœ€å¾Œ', 'é‚£æ™‚å€™', 'ç¾åœ¨']
    comparison_markers = ['æ¯”èµ·', 'ç›¸æ¯”', 'ä¸åƒ', 'è·Ÿ', 'å’Œ...ä¸åŒ']
    problem_markers = ['é‡åˆ°', 'å•é¡Œ', 'å›°é›£', 'è§£æ±º', 'è™•ç†']
    list_markers = ['é¦–å…ˆ', 'ç¬¬ä¸€', 'å…¶æ¬¡', 'ç¬¬äºŒ', 'å†ä¾†', 'æœ€å¾Œ']

    for resp in responses:
        answer = resp['answer']

        chron_count = sum(1 for m in chronological_markers if m in answer)
        if chron_count >= 2:
            patterns['chronological'] += 1

        comp_count = sum(1 for m in comparison_markers if m in answer)
        if comp_count >= 1:
            patterns['comparison'] += 1

        prob_count = sum(1 for m in problem_markers if m in answer)
        if prob_count >= 2:
            patterns['problem_solution'] += 1

        list_count = sum(1 for m in list_markers if m in answer)
        if list_count >= 2:
            patterns['list_style'] += 1

    return patterns


def analyze_answer_length(responses: List[Dict]) -> Dict[str, Any]:
    """åˆ†æå›ç­”é•·åº¦åˆ†å¸ƒ"""
    lengths = [len(resp['answer']) for resp in responses if resp['answer']]

    if not lengths:
        return {}

    return {
        'min': min(lengths),
        'max': max(lengths),
        'avg': sum(lengths) / len(lengths),
        'median': sorted(lengths)[len(lengths) // 2],
        'distribution': {
            'short (<100)': sum(1 for l in lengths if l < 100),
            'medium (100-300)': sum(1 for l in lengths if 100 <= l < 300),
            'long (300-500)': sum(1 for l in lengths if 300 <= l < 500),
            'very_long (>500)': sum(1 for l in lengths if l >= 500),
        }
    }


def find_similar_responses(responses: List[Dict], threshold: float = 0.5) -> List[tuple]:
    """æ‰¾å‡ºç›¸ä¼¼çš„å›ç­”ï¼ˆç°¡å–®çš„å­—ç¬¦é‡ç–Šæ¯”è¼ƒï¼‰"""
    similar_pairs = []

    def simple_similarity(s1: str, s2: str) -> float:
        """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç°¡å–®ç›¸ä¼¼åº¦ï¼ˆå…±åŒå­—ç¬¦æ¯”ä¾‹ï¼‰"""
        if not s1 or not s2:
            return 0
        set1 = set(s1)
        set2 = set(s2)
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0

    # åªæ¯”è¼ƒåŒä¸€å•é¡Œçš„å›ç­”
    question_groups = {}
    for resp in responses:
        q = resp['question']
        if q not in question_groups:
            question_groups[q] = []
        question_groups[q].append(resp)

    for question, group in question_groups.items():
        if len(group) < 2:
            continue
        for i in range(len(group)):
            for j in range(i + 1, len(group)):
                sim = simple_similarity(group[i]['answer'], group[j]['answer'])
                if sim >= threshold:
                    similar_pairs.append({
                        'question': question[:50] + '...' if len(question) > 50 else question,
                        'persona1': group[i]['persona_name'],
                        'persona2': group[j]['persona_name'],
                        'similarity': round(sim, 3),
                        'answer1_preview': group[i]['answer'][:100] + '...',
                        'answer2_preview': group[j]['answer'][:100] + '...',
                    })

    return sorted(similar_pairs, key=lambda x: x['similarity'], reverse=True)[:20]


def generate_report() -> str:
    """ç”Ÿæˆå®Œæ•´çš„å¤šæ¨£æ€§åˆ†æå ±å‘Š"""
    responses = load_all_responses()

    if not responses:
        return "æ²’æœ‰æ‰¾åˆ°ä»»ä½•å›ç­”è³‡æ–™"

    report = []
    report.append("=" * 60)
    report.append("ğŸ“Š å›ç­”å¤šæ¨£æ€§åˆ†æå ±å‘Š")
    report.append("=" * 60)
    report.append(f"\nç¸½å›ç­”æ•¸: {len(responses)}")

    # 1. é–‹é ­æ¨¡å¼åˆ†æ
    report.append("\n" + "-" * 40)
    report.append("ğŸ”¤ é–‹é ­ç”¨èªåˆ†æ (å‰20å­—)")
    report.append("-" * 40)
    opening = analyze_opening_patterns(responses)
    for pattern, count in opening.items():
        if count > 0:
            pct = count / len(responses) * 100
            bar = "â–ˆ" * int(pct / 5)
            report.append(f"  '{pattern}': {count} ({pct:.1f}%) {bar}")

    # 2. çµå°¾æ¨¡å¼åˆ†æ
    report.append("\n" + "-" * 40)
    report.append("ğŸ”š çµå°¾ç”¨èªåˆ†æ (å¾Œ30å­—)")
    report.append("-" * 40)
    ending = analyze_ending_patterns(responses)
    for pattern, count in ending.items():
        if count > 0:
            pct = count / len(responses) * 100
            bar = "â–ˆ" * int(pct / 5)
            report.append(f"  '{pattern}': {count} ({pct:.1f}%) {bar}")

    # 3. æƒ…æ„Ÿè©å½™åˆ†æ
    report.append("\n" + "-" * 40)
    report.append("ğŸ’­ æƒ…æ„Ÿè©å½™åˆ†æ")
    report.append("-" * 40)
    sentiment = analyze_sentiment_words(responses)
    report.append(f"  æ­£é¢è©ç¸½æ•¸: {sentiment['total_positive']}")
    report.append(f"  è² é¢è©ç¸½æ•¸: {sentiment['total_negative']}")
    report.append(f"  ä¸­æ€§è©ç¸½æ•¸: {sentiment['total_neutral']}")
    report.append(f"  æ­£/è² æ¯”: {sentiment['total_positive'] / max(sentiment['total_negative'], 1):.2f}")
    report.append("\n  Top æ­£é¢è©:")
    for word, count in sentiment['positive'].items():
        report.append(f"    {word}: {count}")
    report.append("\n  Top è² é¢è©:")
    for word, count in sentiment['negative'].items():
        report.append(f"    {word}: {count}")

    # 4. çµæ§‹æ¨¡å¼åˆ†æ
    report.append("\n" + "-" * 40)
    report.append("ğŸ“ å›ç­”çµæ§‹æ¨¡å¼")
    report.append("-" * 40)
    structure = analyze_structure_patterns(responses)
    for pattern, count in structure.items():
        pct = count / len(responses) * 100
        report.append(f"  {pattern}: {count} ({pct:.1f}%)")

    # 5. é•·åº¦åˆ†æ
    report.append("\n" + "-" * 40)
    report.append("ğŸ“ å›ç­”é•·åº¦åˆ†æ")
    report.append("-" * 40)
    length = analyze_answer_length(responses)
    if length:
        report.append(f"  æœ€çŸ­: {length['min']} å­—")
        report.append(f"  æœ€é•·: {length['max']} å­—")
        report.append(f"  å¹³å‡: {length['avg']:.1f} å­—")
        report.append(f"  ä¸­ä½æ•¸: {length['median']} å­—")
        report.append("\n  åˆ†å¸ƒ:")
        for bucket, count in length['distribution'].items():
            pct = count / len(responses) * 100
            bar = "â–ˆ" * int(pct / 5)
            report.append(f"    {bucket}: {count} ({pct:.1f}%) {bar}")

    # 6. ç›¸ä¼¼å›ç­”
    report.append("\n" + "-" * 40)
    report.append("ğŸ”„ é«˜åº¦ç›¸ä¼¼çš„å›ç­” (ç›¸ä¼¼åº¦ > 50%)")
    report.append("-" * 40)
    similar = find_similar_responses(responses, threshold=0.5)
    if similar:
        for pair in similar[:10]:
            report.append(f"\n  [{pair['persona1']}] vs [{pair['persona2']}] - ç›¸ä¼¼åº¦: {pair['similarity']}")
            report.append(f"  å•é¡Œ: {pair['question']}")
            report.append(f"  å›ç­”1: {pair['answer1_preview']}")
            report.append(f"  å›ç­”2: {pair['answer2_preview']}")
    else:
        report.append("  æœªç™¼ç¾é«˜åº¦ç›¸ä¼¼çš„å›ç­”")

    report.append("\n" + "=" * 60)
    report.append("åˆ†æå®Œæˆ")
    report.append("=" * 60)

    return "\n".join(report)


if __name__ == "__main__":
    print(generate_report())
