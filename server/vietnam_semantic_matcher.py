"""
èªç¾©å•é¡Œæ¯”å°å™¨
ä½¿ç”¨ OpenAI Embeddings è¨ˆç®—å•é¡Œä¹‹é–“çš„èªç¾©ç›¸ä¼¼åº¦ï¼Œè‡ªå‹•åˆä½µç›¸ä¼¼å•é¡Œ
"""
import os
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ç›¸ä¼¼åº¦é–¾å€¼ - é«˜æ–¼æ­¤å€¼è¦–ç‚ºç›¸åŒå•é¡Œ
# 0.85 å¤ªåš´æ ¼ï¼ˆã€Œè«‹æ¦‚è¿°è‡ªå·±çš„æ—…éŠç¿’æ…£ã€å’Œã€Œç›®å‰ä½ çš„æ—…éŠç¿’æ…£æ˜¯ä»€éº¼ã€åªæœ‰ ~0.73ï¼‰
# 0.72 èƒ½æ•æ‰åˆ°èªç¾©ç›¸ä¼¼ä½†æªè¾­ä¸åŒçš„å•é¡Œ
SIMILARITY_THRESHOLD = 0.72


def get_embedding(text: str) -> List[float]:
    """å–å¾—æ–‡å­—çš„ embedding å‘é‡"""
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding


def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è¨ˆç®—å…©å€‹å‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦"""
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = sum(a * a for a in vec1) ** 0.5
    norm2 = sum(b * b for b in vec2) ** 0.5
    if norm1 == 0 or norm2 == 0:
        return 0
    return dot_product / (norm1 * norm2)


def normalize_question_basic(question: str) -> str:
    """åŸºç¤æ–‡å­—æ­£è¦åŒ–ï¼ˆç§»é™¤æ¨™é»ã€ç©ºç™½ç­‰ï¼‰"""
    import re
    # ç§»é™¤å¼•è™Ÿ
    question = re.sub(r'["ã€Œã€"ã€ã€"]', '', question)
    # ç§»é™¤å•è™Ÿ
    question = re.sub(r'[ï¼Ÿ?]', '', question)
    # ç§»é™¤å¤šé¤˜ç©ºç™½
    question = re.sub(r'\s+', '', question)
    return question.strip()


def group_similar_questions(
    questions: List[str],
    threshold: float = SIMILARITY_THRESHOLD
) -> Dict[str, List[str]]:
    """
    å°‡èªç¾©ç›¸ä¼¼çš„å•é¡Œåˆ†çµ„

    Args:
        questions: å•é¡Œåˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é–¾å€¼

    Returns:
        {
            "canonical_question": ["similar_question1", "similar_question2", ...]
        }
    """
    if not questions:
        return {}

    # å…ˆåšåŸºç¤æ­£è¦åŒ–å»é‡
    normalized_map: Dict[str, str] = {}  # normalized -> first original
    unique_questions: List[str] = []

    for q in questions:
        normalized = normalize_question_basic(q)
        if normalized not in normalized_map:
            normalized_map[normalized] = q
            unique_questions.append(q)

    if len(unique_questions) <= 1:
        return {unique_questions[0]: questions} if unique_questions else {}

    print(f"ğŸ” [Semantic Matcher] Computing embeddings for {len(unique_questions)} unique questions...")

    # è¨ˆç®—æ‰€æœ‰å•é¡Œçš„ embedding
    embeddings: List[List[float]] = []
    for q in unique_questions:
        try:
            emb = get_embedding(q)
            embeddings.append(emb)
        except Exception as e:
            print(f"  âš ï¸ Failed to get embedding for: {q[:30]}... - {e}")
            embeddings.append([])

    # åˆ†çµ„ç›¸ä¼¼å•é¡Œ
    groups: Dict[str, List[str]] = {}
    used: set = set()

    for i, q1 in enumerate(unique_questions):
        if i in used or not embeddings[i]:
            continue

        # å»ºç«‹æ–°ç¾¤çµ„ï¼Œä»¥æ­¤å•é¡Œç‚ºä»£è¡¨
        group = [q1]
        used.add(i)

        for j, q2 in enumerate(unique_questions):
            if j in used or j <= i or not embeddings[j]:
                continue

            similarity = cosine_similarity(embeddings[i], embeddings[j])
            if similarity >= threshold:
                group.append(q2)
                used.add(j)
                print(f"  âœ“ Merged: '{q1[:25]}...' â‰ˆ '{q2[:25]}...' (sim={similarity:.3f})")

        groups[q1] = group

    # å°‡åŸå§‹å•é¡Œï¼ˆåŒ…æ‹¬åŸºç¤æ­£è¦åŒ–å¾Œç›¸åŒçš„ï¼‰æ˜ å°„å›ç¾¤çµ„
    final_groups: Dict[str, List[str]] = {}
    for canonical, similar_list in groups.items():
        all_originals = []
        for similar_q in similar_list:
            # æ‰¾å‡ºæ‰€æœ‰åŸºç¤æ­£è¦åŒ–å¾Œç­‰æ–¼ similar_q çš„åŸå§‹å•é¡Œ
            similar_normalized = normalize_question_basic(similar_q)
            for orig_q in questions:
                if normalize_question_basic(orig_q) == similar_normalized:
                    if orig_q not in all_originals:
                        all_originals.append(orig_q)
        final_groups[canonical] = all_originals if all_originals else similar_list

    print(f"âœ“ [Semantic Matcher] Grouped {len(questions)} questions into {len(final_groups)} groups")

    return final_groups


def find_canonical_question(
    question: str,
    existing_questions: List[str],
    threshold: float = SIMILARITY_THRESHOLD
) -> Tuple[str, float]:
    """
    æ‰¾å‡ºèˆ‡è¼¸å…¥å•é¡Œæœ€ç›¸ä¼¼çš„ç¾æœ‰å•é¡Œ

    Args:
        question: è¦æ¯”å°çš„å•é¡Œ
        existing_questions: ç¾æœ‰å•é¡Œåˆ—è¡¨
        threshold: ç›¸ä¼¼åº¦é–¾å€¼

    Returns:
        (canonical_question, similarity_score)
        å¦‚æœæ‰¾ä¸åˆ°ç›¸ä¼¼å•é¡Œï¼Œè¿”å›åŸå•é¡Œå’Œ 1.0
    """
    if not existing_questions:
        return question, 1.0

    try:
        query_embedding = get_embedding(question)
    except Exception as e:
        print(f"âš ï¸ [Semantic Matcher] Failed to get embedding: {e}")
        return question, 1.0

    best_match = question
    best_score = 0.0

    for existing_q in existing_questions:
        try:
            existing_emb = get_embedding(existing_q)
            similarity = cosine_similarity(query_embedding, existing_emb)
            if similarity > best_score:
                best_score = similarity
                best_match = existing_q
        except Exception:
            continue

    if best_score >= threshold:
        return best_match, best_score

    return question, 1.0


# å¿«å–æ©Ÿåˆ¶ - é¿å…é‡è¤‡è¨ˆç®— embedding
_embedding_cache: Dict[str, List[float]] = {}


def get_embedding_cached(text: str) -> List[float]:
    """å¸¶å¿«å–çš„ embedding å–å¾—"""
    if text not in _embedding_cache:
        _embedding_cache[text] = get_embedding(text)
    return _embedding_cache[text]


def clear_embedding_cache():
    """æ¸…é™¤ embedding å¿«å–"""
    global _embedding_cache
    _embedding_cache = {}


# æ¸¬è©¦
if __name__ == "__main__":
    test_questions = [
        "è«‹æ¦‚è¿°è‡ªå·±çš„æ—…éŠç¿’æ…£èˆ‡å‹æ…‹",
        "ç›®å‰ä½ çš„æ—…éŠç¿’æ…£æ˜¯ä»€éº¼",
        "ä½ å¹³å¸¸çš„æ—…éŠç¿’æ…£è·Ÿå‹æ…‹æ˜¯æ€æ¨£",
        "è²·æ—…éŠéšªçš„ç¶“é©—",
        "æœ‰æ²’æœ‰è³¼è²·æ—…éŠä¿éšªçš„ç¶“é©—",
        "ä½ è²·éæ—…éŠéšªå—",
    ]

    groups = group_similar_questions(test_questions, threshold=0.8)
    print("\nçµæœ:")
    for canonical, similar in groups.items():
        print(f"  [{canonical[:30]}...]")
        for s in similar:
            print(f"    - {s}")
