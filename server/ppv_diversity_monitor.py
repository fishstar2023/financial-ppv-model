"""
PPV (Persona Personality Variance) å¤šå…ƒåŒ–ç›£æ¸¬ç³»çµ±
ä½¿ç”¨ Core/Style è§£è€¦æ–¹æ³•ä¾†æª¢æ¸¬çœŸå¯¦å¤šæ¨£æ€§

åŸºæ–¼å®Œæ•´ PPV Schema æ¬„ä½é€²è¡Œåˆ†æï¼š

ã€äººæ ¼ç‰¹è³ªã€‘
- big5: openness, conscientiousness, extraversion, agreeableness, neuroticism (0-100)
- hexaco: honesty_humility, emotionality, extraversion, agreeableness, conscientiousness, openness (0-100)
- disc: dominance, influence, steadiness, conscientiousness (0-100)
- mbti: E_I, S_N, T_F, J_P (-100 to 100)
- enneagram: primary_type (1-9), wing (1-9), instinct (sp/so/sx)

ã€åƒ¹å€¼è§€èˆ‡é“å¾·ã€‘
- schwartz_values: 10ç¨®åƒ¹å€¼è§€ (0-100)
- moral_foundations: Care, Fairness, Loyalty, Authority, Sanctity (0-100)

ã€æ±ºç­–é¢¨æ ¼ã€‘
- risk_profile: overall, financial, ethical, social, health (0-100)
- time_preference: discount_rate, planning_horizon (0-100), present_vs_future (-100 to 100)
- regulatory_focus: promotion, prevention (0-100)
- decision_style: primary, secondary, risk_seeking, info_processing, social_preference

ã€æºé€šé¢¨æ ¼ã€‘
- language_style: formality, directness, emotion_expression, verbosity, questioning_style (0-100)
- emotion_profile: baseline_valence, emotional_range, stress_response, recovery_speed (0-100)
- social_profile: trust_default, cooperation_tendency, conformity, independence (0-100)

ã€è¡Œç‚ºæŒ‡æ¨™ã€‘
- behavioral_indicators: information_seeking, comparison_behavior, price_sensitivity, brand_loyalty (0-100)

ç›£æ¸¬æŒ‡æ¨™:
- ESS (Effective Sample Size): æœ‰æ•ˆæ¨£æœ¬æ•¸ â†‘
- Mean Min-Distance: å¹³å‡æœ€å°è·é›¢ â†‘
- Occupied Bins: ä½”ç”¨æ ¼æ•¸ â†‘
- Min Entropy: å„ç¶­åº¦æœ€å°ç†µ â‰¥ Ï„
"""

import numpy as np
from typing import List, Dict, Any, Tuple
from collections import Counter
import json
from datetime import datetime

# ========== PPV Trait Dimensions å®šç¾© ==========
# åŸºæ–¼å®Œæ•´ PPV Schema

def _create_continuous_dim(name: str, range_vals: list = None, bins: list = None) -> dict:
    """è¼”åŠ©å‡½æ•¸ï¼šå»ºç«‹é€£çºŒå‹ç¶­åº¦å®šç¾©"""
    return {
        "name": name,
        "type": "continuous",
        "range": range_vals or [0, 100],
        "bins": bins or ["LOW (0-33)", "MEDIUM (34-66)", "HIGH (67-100)"]
    }

def _create_categorical_dim(name: str, values: list) -> dict:
    """è¼”åŠ©å‡½æ•¸ï¼šå»ºç«‹é¡åˆ¥å‹ç¶­åº¦å®šç¾©"""
    return {
        "name": name,
        "type": "categorical",
        "values": values
    }

# Core Traits - å½±éŸ¿å¯¦éš›æ±ºç­–è¡Œç‚ºçš„ç¶­åº¦
CORE_TRAIT_DIMENSIONS = {
    # === Big5 äººæ ¼ ===
    "big5_openness": _create_continuous_dim("é–‹æ”¾æ€§ (Openness)"),
    "big5_conscientiousness": _create_continuous_dim("ç›¡è²¬æ€§ (Conscientiousness)"),
    "big5_extraversion": _create_continuous_dim("å¤–å‘æ€§ (Extraversion)"),
    "big5_agreeableness": _create_continuous_dim("è¦ªå’Œæ€§ (Agreeableness)"),
    "big5_neuroticism": _create_continuous_dim("ç¥ç¶“è³ª (Neuroticism)"),

    # === Risk Profile ===
    "risk_overall": _create_continuous_dim("æ•´é«”é¢¨éšªæ‰¿å—"),
    "risk_financial": _create_continuous_dim("è²¡å‹™é¢¨éšªæ‰¿å—"),
    "risk_ethical": _create_continuous_dim("å€«ç†é¢¨éšªæ‰¿å—"),
    "risk_social": _create_continuous_dim("ç¤¾äº¤é¢¨éšªæ‰¿å—"),
    "risk_health": _create_continuous_dim("å¥åº·é¢¨éšªæ‰¿å—"),

    # === Decision Style ===
    "decision_primary": _create_categorical_dim("ä¸»è¦æ±ºç­–é¢¨æ ¼",
        ["analytical", "intuitive", "dependent", "avoidant", "spontaneous"]),
    "decision_risk_seeking": _create_continuous_dim("é¢¨éšªå°‹æ±‚å‚¾å‘"),
    "decision_info_processing": _create_categorical_dim("è³‡è¨Šè™•ç†æ–¹å¼",
        ["maximizer", "satisficer", "optimizer"]),

    # === Time Preference ===
    "time_discount_rate": _create_continuous_dim("æ™‚é–“æŠ˜æ‰£ç‡"),
    "time_planning_horizon": _create_categorical_dim("è¦åŠƒæ™‚é–“ç¯„åœ",
        ["immediate", "short_term", "medium_term", "long_term"]),
    "time_present_vs_future": _create_continuous_dim("ç¾åœ¨vsæœªä¾†å°å‘", [-100, 100]),

    # === Regulatory Focus ===
    "regulatory_promotion": _create_continuous_dim("ä¿ƒé€²ç„¦é»"),
    "regulatory_prevention": _create_continuous_dim("é é˜²ç„¦é»"),
}

# Extended Traits - HEXACO, DISC, MBTI, Enneagram
EXTENDED_PERSONALITY_DIMENSIONS = {
    # === HEXACO ===
    "hexaco_honesty_humility": _create_continuous_dim("èª å¯¦-è¬™éœ"),
    "hexaco_emotionality": _create_continuous_dim("æƒ…ç·’æ€§"),
    "hexaco_extraversion": _create_continuous_dim("å¤–å‘æ€§ (HEXACO)"),
    "hexaco_agreeableness": _create_continuous_dim("è¦ªå’Œæ€§ (HEXACO)"),
    "hexaco_conscientiousness": _create_continuous_dim("ç›¡è²¬æ€§ (HEXACO)"),
    "hexaco_openness": _create_continuous_dim("ç¶“é©—é–‹æ”¾æ€§ (HEXACO)"),

    # === DISC ===
    "disc_dominance": _create_continuous_dim("æ”¯é…æ€§ (D)"),
    "disc_influence": _create_continuous_dim("å½±éŸ¿åŠ› (I)"),
    "disc_steadiness": _create_continuous_dim("ç©©å®šæ€§ (S)"),
    "disc_conscientiousness": _create_continuous_dim("è¬¹æ…æ€§ (C)"),

    # === MBTI ===
    "mbti_E_I": _create_continuous_dim("å¤–å‘-å…§å‘ (E-I)", [-100, 100]),
    "mbti_S_N": _create_continuous_dim("å¯¦æ„Ÿ-ç›´è¦º (S-N)", [-100, 100]),
    "mbti_T_F": _create_continuous_dim("æ€è€ƒ-æƒ…æ„Ÿ (T-F)", [-100, 100]),
    "mbti_J_P": _create_continuous_dim("åˆ¤æ–·-æ„ŸçŸ¥ (J-P)", [-100, 100]),

    # === Enneagram ===
    "enneagram_primary": _create_categorical_dim("ä¹å‹äººæ ¼ä¸»å‹",
        ["1", "2", "3", "4", "5", "6", "7", "8", "9"]),
    "enneagram_instinct": _create_categorical_dim("æœ¬èƒ½è®Šé«”",
        ["sp", "so", "sx"]),
}

# Style Traits - æºé€šèˆ‡ç¤¾äº¤é¢¨æ ¼
STYLE_TRAIT_DIMENSIONS = {
    # === Language Style ===
    "language_formality": _create_continuous_dim("æ­£å¼ç¨‹åº¦"),
    "language_directness": _create_continuous_dim("ç›´æ¥ç¨‹åº¦"),
    "language_emotion_expression": _create_continuous_dim("æƒ…ç·’è¡¨é”"),
    "language_verbosity": _create_continuous_dim("å†—é•·ç¨‹åº¦"),

    # === Emotion Profile ===
    "emotion_baseline_valence": _create_continuous_dim("åŸºæº–æƒ…ç·’æ•ˆåƒ¹"),
    "emotion_range": _create_continuous_dim("æƒ…ç·’ç¯„åœ"),
    "emotion_stress_response": _create_categorical_dim("å£“åŠ›åæ‡‰",
        ["fight", "flight", "freeze", "fawn"]),

    # === Social Profile ===
    "social_trust_default": _create_continuous_dim("é è¨­ä¿¡ä»»åº¦"),
    "social_cooperation": _create_continuous_dim("åˆä½œå‚¾å‘"),
    "social_conformity": _create_continuous_dim("å¾çœ¾å‚¾å‘"),
    "social_independence": _create_continuous_dim("ç¨ç«‹æ€§"),

    # === Moral Foundations ===
    "moral_care": _create_continuous_dim("é“å¾·-é—œæ‡·"),
    "moral_fairness": _create_continuous_dim("é“å¾·-å…¬å¹³"),
    "moral_loyalty": _create_continuous_dim("é“å¾·-å¿ èª "),
    "moral_authority": _create_continuous_dim("é“å¾·-æ¬Šå¨"),
    "moral_sanctity": _create_continuous_dim("é“å¾·-è–æ½”"),
}

# Behavioral Indicators - è¡Œç‚ºæŒ‡æ¨™
BEHAVIORAL_DIMENSIONS = {
    "behavior_info_seeking": _create_continuous_dim("è³‡è¨Šæœå°‹å‚¾å‘"),
    "behavior_comparison": _create_continuous_dim("æ¯”è¼ƒè¡Œç‚º"),
    "behavior_price_sensitivity": _create_continuous_dim("åƒ¹æ ¼æ•æ„Ÿåº¦"),
    "behavior_brand_loyalty": _create_continuous_dim("å“ç‰Œå¿ èª åº¦"),
}


class PPVDiversityMonitor:
    """PPV å¤šå…ƒåŒ–ç›£æ¸¬å™¨ - åŸºæ–¼å®Œæ•´ PPV Schema"""

    def __init__(self, entropy_threshold: float = 0.5, use_extended: bool = False):
        self.entropy_threshold = entropy_threshold
        self.core_dimensions = CORE_TRAIT_DIMENSIONS
        self.extended_dimensions = EXTENDED_PERSONALITY_DIMENSIONS
        self.style_dimensions = STYLE_TRAIT_DIMENSIONS
        self.behavioral_dimensions = BEHAVIORAL_DIMENSIONS
        self.use_extended = use_extended

    def extract_core_traits(self, persona: Dict[str, Any]) -> Dict[str, Any]:
        """å¾ persona æå– core traits (å®Œæ•´ PPV Schema)"""
        traits = {}

        # === Big5 ===
        big5 = persona.get('big5', {})
        traits['big5_openness'] = big5.get('openness', 50)
        traits['big5_conscientiousness'] = big5.get('conscientiousness', 50)
        traits['big5_extraversion'] = big5.get('extraversion', 50)
        traits['big5_agreeableness'] = big5.get('agreeableness', 50)
        traits['big5_neuroticism'] = big5.get('neuroticism', 50)

        # === Risk Profile ===
        risk = persona.get('risk_profile', {})
        traits['risk_overall'] = risk.get('overall', 50)
        traits['risk_financial'] = risk.get('financial', 50)
        traits['risk_ethical'] = risk.get('ethical', 50)
        traits['risk_social'] = risk.get('social', 50)
        traits['risk_health'] = risk.get('health', 50)

        # === Decision Style ===
        decision = persona.get('decision_style', {})
        if isinstance(decision, str):
            # èˆŠæ ¼å¼ï¼šdecision_style æ˜¯å­—ä¸²
            traits['decision_primary'] = decision.lower()
        else:
            # æ–°æ ¼å¼ï¼šdecision_style æ˜¯ç‰©ä»¶
            traits['decision_primary'] = decision.get('primary', 'intuitive').lower()
            traits['decision_risk_seeking'] = decision.get('risk_seeking', 50)
            traits['decision_info_processing'] = decision.get('info_processing', 'satisficer')

        # === Time Preference ===
        time_pref = persona.get('time_preference', {})
        traits['time_discount_rate'] = time_pref.get('discount_rate', 50)
        traits['time_planning_horizon'] = time_pref.get('planning_horizon', 'medium_term')
        traits['time_present_vs_future'] = time_pref.get('present_vs_future', 0)

        # === Regulatory Focus ===
        reg_focus = persona.get('regulatory_focus', {})
        traits['regulatory_promotion'] = reg_focus.get('promotion', 50)
        traits['regulatory_prevention'] = reg_focus.get('prevention', 50)

        return traits

    def extract_extended_traits(self, persona: Dict[str, Any]) -> Dict[str, Any]:
        """å¾ persona æå–æ“´å±•äººæ ¼ç‰¹è³ª (HEXACO, DISC, MBTI, Enneagram)"""
        traits = {}

        # === HEXACO ===
        hexaco = persona.get('hexaco', {})
        traits['hexaco_honesty_humility'] = hexaco.get('honesty_humility', 50)
        traits['hexaco_emotionality'] = hexaco.get('emotionality', 50)
        traits['hexaco_extraversion'] = hexaco.get('extraversion', 50)
        traits['hexaco_agreeableness'] = hexaco.get('agreeableness', 50)
        traits['hexaco_conscientiousness'] = hexaco.get('conscientiousness', 50)
        traits['hexaco_openness'] = hexaco.get('openness', 50)

        # === DISC ===
        disc = persona.get('disc', {})
        traits['disc_dominance'] = disc.get('dominance', 50)
        traits['disc_influence'] = disc.get('influence', 50)
        traits['disc_steadiness'] = disc.get('steadiness', 50)
        traits['disc_conscientiousness'] = disc.get('conscientiousness', 50)

        # === MBTI ===
        mbti = persona.get('mbti', {})
        traits['mbti_E_I'] = mbti.get('E_I', 0)
        traits['mbti_S_N'] = mbti.get('S_N', 0)
        traits['mbti_T_F'] = mbti.get('T_F', 0)
        traits['mbti_J_P'] = mbti.get('J_P', 0)

        # === Enneagram ===
        enneagram = persona.get('enneagram', {})
        traits['enneagram_primary'] = str(enneagram.get('primary_type', '5'))
        traits['enneagram_instinct'] = enneagram.get('instinct', 'sp')

        return traits

    def extract_style_traits(self, persona: Dict[str, Any]) -> Dict[str, Any]:
        """å¾ persona æå– style traits (æºé€šèˆ‡ç¤¾äº¤é¢¨æ ¼)"""
        traits = {}

        # === Language Style ===
        lang = persona.get('language_style', {})
        traits['language_formality'] = lang.get('formality', 50)
        traits['language_directness'] = lang.get('directness', 50)
        traits['language_emotion_expression'] = lang.get('emotion_expression', 50)
        traits['language_verbosity'] = lang.get('verbosity', 50)

        # === Emotion Profile ===
        emotion = persona.get('emotion_profile', {})
        traits['emotion_baseline_valence'] = emotion.get('baseline_valence', 50)
        traits['emotion_range'] = emotion.get('emotional_range', 50)
        traits['emotion_stress_response'] = emotion.get('stress_response', 'flight')

        # === Social Profile ===
        social = persona.get('social_profile', {})
        traits['social_trust_default'] = social.get('trust_default', 50)
        traits['social_cooperation'] = social.get('cooperation_tendency', 50)
        traits['social_conformity'] = social.get('conformity', 50)
        traits['social_independence'] = social.get('independence', 50)

        # === Moral Foundations ===
        moral = persona.get('moral_foundations', {})
        traits['moral_care'] = moral.get('Care', 50)
        traits['moral_fairness'] = moral.get('Fairness', 50)
        traits['moral_loyalty'] = moral.get('Loyalty', 50)
        traits['moral_authority'] = moral.get('Authority', 50)
        traits['moral_sanctity'] = moral.get('Sanctity', 50)

        return traits

    def extract_behavioral_traits(self, persona: Dict[str, Any]) -> Dict[str, Any]:
        """å¾ persona æå–è¡Œç‚ºæŒ‡æ¨™"""
        traits = {}

        behavior = persona.get('behavioral_indicators', {})
        traits['behavior_info_seeking'] = behavior.get('information_seeking', 50)
        traits['behavior_comparison'] = behavior.get('comparison_behavior', 50)
        traits['behavior_price_sensitivity'] = behavior.get('price_sensitivity', 50)
        traits['behavior_brand_loyalty'] = behavior.get('brand_loyalty', 50)

        return traits

    def check_ppv_completeness(self, persona: Dict[str, Any]) -> Dict[str, Any]:
        """æª¢æŸ¥ persona çš„ PPV æ¬„ä½å®Œæ•´æ€§"""
        required_fields = ['big5', 'risk_profile', 'decision_style']
        optional_core = ['time_preference', 'regulatory_focus']
        extended_fields = ['hexaco', 'disc', 'mbti', 'enneagram']
        style_fields = ['language_style', 'emotion_profile', 'social_profile', 'moral_foundations']
        behavioral_fields = ['behavioral_indicators']

        result = {
            'has_basic': all(f in persona for f in required_fields),
            'has_extended_core': any(f in persona for f in optional_core),
            'has_extended_personality': any(f in persona for f in extended_fields),
            'has_style': any(f in persona for f in style_fields),
            'has_behavioral': any(f in persona for f in behavioral_fields),
            'missing_required': [f for f in required_fields if f not in persona],
            'present_optional': [f for f in optional_core + extended_fields + style_fields + behavioral_fields if f in persona]
        }

        # è¨ˆç®—å®Œæ•´åº¦åˆ†æ•¸
        total_fields = len(required_fields) + len(optional_core) + len(extended_fields) + len(style_fields) + len(behavioral_fields)
        present_count = len([f for f in required_fields + optional_core + extended_fields + style_fields + behavioral_fields if f in persona])
        result['completeness_score'] = round(present_count / total_fields, 2)

        return result

    def _value_to_bin(self, value: float, num_bins: int = 3) -> int:
        """å°‡é€£çºŒå€¼è½‰æ›ç‚º bin index"""
        if value <= 33:
            return 0
        elif value <= 66:
            return 1
        else:
            return 2

    def _value_to_bin_label(self, value: float) -> str:
        """å°‡é€£çºŒå€¼è½‰æ›ç‚º bin æ¨™ç±¤"""
        if value <= 33:
            return "LOW (0-33)"
        elif value <= 66:
            return "MEDIUM (34-66)"
        else:
            return "HIGH (67-100)"

    def compute_diversity_metrics(self, personas: List[Dict[str, Any]]) -> Dict[str, Any]:
        """è¨ˆç®—æ‰€æœ‰å¤šæ¨£æ€§æŒ‡æ¨™"""
        if not personas:
            return {"error": "No personas provided"}

        # æª¢æŸ¥ PPV å®Œæ•´æ€§
        completeness_results = [self.check_ppv_completeness(p) for p in personas]
        personas_with_basic = [p for p, c in zip(personas, completeness_results) if c['has_basic']]
        personas_missing_ppv = [p for p, c in zip(personas, completeness_results) if not c['has_basic']]

        # è¨ˆç®—å¹³å‡å®Œæ•´åº¦
        avg_completeness = np.mean([c['completeness_score'] for c in completeness_results])

        if not personas_with_basic:
            return {
                "error": "All personas missing required PPV fields (big5, risk_profile, decision_style)",
                "hint": "Please ensure personas have at least big5, risk_profile, and decision_style fields",
                "missing_count": len(personas),
                "completeness_report": completeness_results
            }

        # æå–æ‰€æœ‰ traits (åªä½¿ç”¨æœ‰ PPV æ¬„ä½çš„ personas)
        all_core_traits = [self.extract_core_traits(p) for p in personas_with_basic]
        all_style_traits = [self.extract_style_traits(p) for p in personas_with_basic]
        all_extended_traits = [self.extract_extended_traits(p) for p in personas_with_basic] if self.use_extended else []
        all_behavioral_traits = [self.extract_behavioral_traits(p) for p in personas_with_basic]

        metrics = {
            "timestamp": datetime.now().isoformat(),
            "total_personas": len(personas),
            "personas_with_ppv": len(personas_with_basic),
            "personas_missing_ppv": len(personas_missing_ppv),
            "avg_completeness": round(avg_completeness, 2),
            "ppv_schema_detected": True,
            "core_metrics": self._compute_trait_metrics(all_core_traits, self.core_dimensions, "core"),
            "style_metrics": self._compute_trait_metrics(all_style_traits, self.style_dimensions, "style"),
            "behavioral_metrics": self._compute_trait_metrics(all_behavioral_traits, self.behavioral_dimensions, "behavioral"),
            "combined_metrics": self._compute_combined_metrics(all_core_traits),
            "trait_summary": self._compute_trait_summary(all_core_traits),
            "completeness_details": {
                "missing_personas": [p.get('id', 'unknown') for p in personas_missing_ppv],
                "sample_missing_fields": completeness_results[0]['missing_required'] if completeness_results else []
            },
            "diversity_health": None
        }

        # å¦‚æœä½¿ç”¨æ“´å±•ç¶­åº¦ï¼ŒåŠ å…¥æ“´å±•æŒ‡æ¨™
        if self.use_extended and all_extended_traits:
            metrics["extended_metrics"] = self._compute_trait_metrics(
                all_extended_traits, self.extended_dimensions, "extended"
            )

        # è¨ˆç®—æ•´é«”å¥åº·åº¦
        metrics["diversity_health"] = self._compute_health_score(metrics)

        return metrics

    def _compute_trait_summary(self, all_traits: List[Dict]) -> Dict:
        """è¨ˆç®—æ¯å€‹ trait çš„çµ±è¨ˆæ‘˜è¦"""
        summary = {}
        n = len(all_traits)

        for dim_key, dim_info in self.core_dimensions.items():
            # å®‰å…¨åœ°å–å¾—å€¼
            values = [t.get(dim_key) for t in all_traits]
            valid_values = [v for v in values if v is not None]
            if not valid_values:
                continue

            if dim_info.get("type") == "categorical":
                # é¡åˆ¥å‹ - è¨ˆç®—åˆ†å¸ƒ
                counter = Counter(valid_values)
                summary[dim_key] = {
                    "name": dim_info["name"],
                    "type": "categorical",
                    "distribution": {k: round(v/len(valid_values), 3) for k, v in counter.items()},
                    "mode": counter.most_common(1)[0][0] if counter else None,
                    "unique_count": len(counter)
                }
            else:
                # é€£çºŒå‹ - è¨ˆç®—çµ±è¨ˆ
                values_array = np.array(valid_values)
                summary[dim_key] = {
                    "name": dim_info["name"],
                    "type": "continuous",
                    "mean": round(float(np.mean(values_array)), 1),
                    "std": round(float(np.std(values_array)), 1),
                    "min": round(float(np.min(values_array)), 1),
                    "max": round(float(np.max(values_array)), 1),
                    "range": round(float(np.max(values_array) - np.min(values_array)), 1)
                }

        return summary

    def _compute_trait_metrics(self, all_traits: List[Dict], dimensions: Dict, trait_type: str) -> Dict:
        """è¨ˆç®—å–®ä¸€é¡å‹ç‰¹è³ªçš„æŒ‡æ¨™"""
        n = len(all_traits)
        dim_metrics = {}
        entropies = []

        for dim_key, dim_info in dimensions.items():
            # å®‰å…¨åœ°å–å¾—å€¼ï¼Œè·³éä¸å­˜åœ¨çš„ç¶­åº¦
            values = [t.get(dim_key) for t in all_traits]
            # éæ¿¾æ‰ None å€¼
            valid_values = [v for v in values if v is not None]
            if not valid_values:
                continue  # è·³éæ²’æœ‰è³‡æ–™çš„ç¶­åº¦
            values = valid_values

            n_valid = len(values)  # ä½¿ç”¨æœ‰æ•ˆæ•¸é‡

            if dim_info.get("type") == "categorical":
                # é¡åˆ¥å‹è®Šæ•¸
                counter = Counter(values)
                all_categories = dim_info.get("values", list(counter.keys()))
                distribution = {v: counter.get(v, 0) / n_valid for v in all_categories}
                occupied_bins = len([v for v in counter.values() if v > 0])
                total_bins = len(all_categories)
            else:
                # é€£çºŒå‹è®Šæ•¸ - è½‰æ›ç‚º bins
                binned_values = [self._value_to_bin_label(v) for v in values]
                counter = Counter(binned_values)
                bins = dim_info.get("bins", ["LOW (0-33)", "MEDIUM (34-66)", "HIGH (67-100)"])
                distribution = {b: counter.get(b, 0) / n_valid for b in bins}
                occupied_bins = len([v for v in counter.values() if v > 0])
                total_bins = len(bins)

            # è¨ˆç®—ç†µ
            entropy = self._compute_entropy(list(counter.values()), n_valid)
            entropies.append(entropy)
            max_entropy = np.log2(total_bins) if total_bins > 1 else 1

            dim_metrics[dim_key] = {
                "name": dim_info["name"],
                "distribution": distribution,
                "entropy": round(entropy, 3),
                "max_entropy": round(max_entropy, 3),
                "normalized_entropy": round(entropy / max_entropy if max_entropy > 0 else 0, 3),
                "occupied_bins": occupied_bins,
                "total_bins": total_bins,
                "bin_coverage": round(occupied_bins / total_bins, 3)
            }

        return {
            "dimensions": dim_metrics,
            "min_entropy": round(float(min(entropies)), 3) if entropies else 0,
            "mean_entropy": round(float(np.mean(entropies)), 3) if entropies else 0,
            "entropy_threshold_met": bool(min(entropies) >= self.entropy_threshold) if entropies else False
        }

    def _compute_combined_metrics(self, all_core_traits: List[Dict]) -> Dict:
        """è¨ˆç®—çµ„åˆæŒ‡æ¨™ï¼ˆESS, Mean Min-Distanceï¼‰"""
        n = len(all_core_traits)

        if n < 2:
            return {
                "ess": n,
                "ess_ratio": 1.0,
                "mean_min_distance": 0,
                "unique_combinations": n,
                "uniqueness_ratio": 1.0,
                "occupied_bins": 1,
                "total_possible_bins": 1,
                "bin_coverage": 1.0
            }

        # æ‰¾å‡ºæ‰€æœ‰ personas éƒ½æœ‰çš„ç¶­åº¦
        available_dims = []
        for dim_key in sorted(self.core_dimensions.keys()):
            if all(traits.get(dim_key) is not None for traits in all_core_traits):
                available_dims.append(dim_key)

        if not available_dims:
            return {
                "ess": n,
                "ess_ratio": 1.0,
                "mean_min_distance": 0,
                "unique_combinations": n,
                "uniqueness_ratio": 1.0,
                "occupied_bins": 1,
                "total_possible_bins": 1,
                "bin_coverage": 1.0,
                "dimensions_used": 0
            }

        # å°‡ traits è½‰æ›ç‚ºå‘é‡ (normalized)
        vectors = []
        for traits in all_core_traits:
            vec = []
            for dim_key in available_dims:
                dim_info = self.core_dimensions[dim_key]
                value = traits.get(dim_key, 50)  # é è¨­å€¼ 50

                if dim_info.get("type") == "categorical":
                    # é¡åˆ¥å‹ - è½‰æ›ç‚º one-hot çš„ index
                    categories = dim_info.get("values", [])
                    idx = categories.index(value) if value in categories else 0
                    normalized = idx / max(len(categories) - 1, 1)
                else:
                    # é€£çºŒå‹ - æ­¸ä¸€åŒ–åˆ° [0, 1]
                    normalized = value / 100.0

                vec.append(normalized)
            vectors.append(vec)

        vectors = np.array(vectors)

        # è¨ˆç®— ESS - ä½¿ç”¨ trait signature çš„å”¯ä¸€æ€§
        trait_signatures = []
        for traits in all_core_traits:
            sig = tuple(
                self._value_to_bin(traits.get(k, 50)) if self.core_dimensions[k].get("type") != "categorical"
                else traits.get(k, 'unknown')
                for k in available_dims
            )
            trait_signatures.append(sig)

        unique_count = len(set(trait_signatures))
        ess = unique_count

        # è¨ˆç®— Mean Min-Distance (Euclidean)
        min_distances = []
        for i in range(n):
            distances = []
            for j in range(n):
                if i != j:
                    dist = np.sqrt(np.sum((vectors[i] - vectors[j]) ** 2))
                    distances.append(dist)
            if distances:
                min_distances.append(min(distances))

        mean_min_dist = np.mean(min_distances) if min_distances else 0

        # è¨ˆç®— occupied binsï¼ˆåœ¨å¤šç¶­ç©ºé–“ä¸­ï¼‰
        grid_size = 3
        bin_coords = []
        for vec in vectors:
            coord = tuple(int(v * grid_size) if v < 1 else grid_size - 1 for v in vec)
            bin_coords.append(coord)

        occupied_bins = len(set(bin_coords))
        total_possible_bins = grid_size ** len(available_dims)

        return {
            "ess": int(ess),
            "ess_ratio": round(float(ess / n), 3),
            "dimensions_used": len(available_dims),
            "mean_min_distance": round(float(mean_min_dist), 4),
            "unique_combinations": int(unique_count),
            "uniqueness_ratio": round(float(unique_count / n), 3),
            "occupied_bins": int(occupied_bins),
            "total_possible_bins": int(total_possible_bins),
            "bin_coverage": round(float(occupied_bins / min(total_possible_bins, n * 2)), 3)
        }

    def _compute_entropy(self, counts: List[int], total: int) -> float:
        """è¨ˆç®— Shannon ç†µ"""
        if total == 0:
            return 0
        probs = [c / total for c in counts if c > 0]
        return -sum(p * np.log2(p) for p in probs)

    def _compute_health_score(self, metrics: Dict) -> Dict:
        """è¨ˆç®—æ•´é«”å¤šæ¨£æ€§å¥åº·åˆ†æ•¸"""
        core = metrics["core_metrics"]
        combined = metrics["combined_metrics"]

        scores = {}
        warnings = []

        # 1. ESS ratio (ç†æƒ³: 1.0 = å®Œç¾ï¼Œ0.7 = åˆæ ¼)
        ess_ratio = combined["ess_ratio"]
        scores["ess"] = ess_ratio  # ç›´æ¥ä½¿ç”¨æ¯”ä¾‹ï¼Œæ›´æœ‰å€åˆ†åº¦
        if ess_ratio < 0.5:
            warnings.append(f"âš ï¸ ESS ratio éä½ ({ess_ratio:.2f})ï¼Œå­˜åœ¨é‡è¤‡ personas")

        # 2. Mean min-distance (ç†æƒ³: > 0.3ï¼Œåˆæ ¼: > 0.15)
        mean_dist = combined["mean_min_distance"]
        # ä½¿ç”¨ sigmoid-like å‡½æ•¸ï¼Œ0.3 ä»¥ä¸Šç‚ºé«˜åˆ†ï¼Œ0.15 ä»¥ä¸‹ç‚ºä½åˆ†
        scores["distance"] = min(mean_dist / 0.35, 1.0)
        if mean_dist < 0.1:
            warnings.append(f"âš ï¸ å¹³å‡æœ€å°è·é›¢éä½ ({mean_dist:.3f})ï¼Œpersonas éæ–¼ç›¸ä¼¼")

        # 3. Bin coverage (ç†æƒ³: > 0.6ï¼Œåˆæ ¼: > 0.3)
        bin_cov = combined["bin_coverage"]
        scores["coverage"] = min(bin_cov / 0.6, 1.0)  # æé«˜æ¨™æº–
        if bin_cov < 0.2:
            warnings.append(f"âš ï¸ Bin coverage éä½ ({bin_cov:.2f})ï¼Œåˆ†å¸ƒéæ–¼é›†ä¸­")

        # 4. Mean normalized entropy (ç†æƒ³: > 0.8ï¼Œåˆæ ¼: > 0.5)
        mean_norm_ent = core["mean_entropy"] / 1.585 if core["mean_entropy"] else 0  # 1.585 = log2(3) æœ€å¤§ç†µ
        scores["entropy"] = mean_norm_ent
        if mean_norm_ent < 0.5:
            warnings.append(f"âš ï¸ å¹³å‡æ­£è¦åŒ–ç†µéä½ ({mean_norm_ent:.2f})ï¼Œåˆ†å¸ƒä¸å¤ å‡å‹»")

        # æ‰¾å‡ºä½å¤šæ¨£æ€§çš„ç¶­åº¦
        low_diversity_dims = []
        for dim_key, dim_metrics in core["dimensions"].items():
            if dim_metrics["normalized_entropy"] < 0.5:
                low_diversity_dims.append({
                    "dimension": dim_metrics["name"],
                    "entropy": dim_metrics["normalized_entropy"],
                    "coverage": dim_metrics["bin_coverage"]
                })

        if low_diversity_dims:
            warnings.append(f"âš ï¸ {len(low_diversity_dims)} å€‹ç¶­åº¦å¤šæ¨£æ€§ä¸è¶³")

        # è¨ˆç®—ç¸½åˆ†
        overall_score = float(np.mean(list(scores.values())))

        # åˆ¤å®šç‹€æ…‹
        if overall_score >= 0.8:
            status = "ğŸŸ¢ HEALTHY"
        elif overall_score >= 0.6:
            status = "ğŸŸ¡ MODERATE"
        elif overall_score >= 0.4:
            status = "ğŸŸ  WARNING"
        else:
            status = "ğŸ”´ CRITICAL"

        return {
            "overall_score": round(float(overall_score), 3),
            "status": status,
            "component_scores": {k: round(float(v), 3) for k, v in scores.items()},
            "warnings": warnings,
            "low_diversity_dimensions": low_diversity_dims,
            "is_real_diversity": bool(overall_score >= 0.6 and not low_diversity_dims)
        }

    def generate_report(self, personas: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆå¯è®€çš„å¤šæ¨£æ€§å ±å‘Š"""
        metrics = self.compute_diversity_metrics(personas)

        if "error" in metrics:
            return f"Error: {metrics['error']}\nHint: {metrics.get('hint', '')}"

        report = []
        report.append("=" * 60)
        report.append("ğŸ“Š PPV å¤šå…ƒåŒ–ç›£æ¸¬å ±å‘Š (åŸºæ–¼çœŸå¯¦ PPV Schema)")
        report.append("=" * 60)
        report.append(f"æ™‚é–“: {metrics['timestamp']}")
        report.append(f"å—æ¸¬ Personas: {metrics['total_personas']}")
        report.append(f"PPV Schema: âœ“ æª¢æ¸¬åˆ°")
        report.append("")

        # å¥åº·ç‹€æ…‹
        health = metrics["diversity_health"]
        report.append(f"æ•´é«”ç‹€æ…‹: {health['status']}")
        report.append(f"ç¸½åˆ†: {health['overall_score']:.1%}")
        report.append("")

        # çµ„ä»¶åˆ†æ•¸
        report.append("çµ„ä»¶åˆ†æ•¸:")
        for k, v in health["component_scores"].items():
            bar = "â–ˆ" * int(v * 10) + "â–‘" * (10 - int(v * 10))
            report.append(f"  {k:12s}: [{bar}] {v:.1%}")
        report.append("")

        # Trait Summary
        report.append("-" * 40)
        report.append("ğŸ“ˆ Trait çµ±è¨ˆæ‘˜è¦:")
        report.append("-" * 40)
        for dim_key, summary in metrics["trait_summary"].items():
            if summary["type"] == "continuous":
                report.append(f"  {summary['name']}: Î¼={summary['mean']}, Ïƒ={summary['std']}, range=[{summary['min']}-{summary['max']}]")
            else:
                dist_str = ", ".join([f"{k}:{int(v*100)}%" for k, v in summary["distribution"].items() if v > 0])
                report.append(f"  {summary['name']}: {dist_str}")
        report.append("")

        # Core Metrics - Entropy
        report.append("-" * 40)
        report.append("ğŸ¯ Core Traits ç†µåˆ†æ:")
        report.append("-" * 40)
        core = metrics["core_metrics"]
        report.append(f"  æœ€å°ç†µ: {core['min_entropy']:.3f} (é–¾å€¼: {self.entropy_threshold})")
        report.append(f"  å¹³å‡ç†µ: {core['mean_entropy']:.3f}")
        report.append(f"  ç†µé”æ¨™: {'âœ“' if core['entropy_threshold_met'] else 'âœ—'}")
        report.append("")

        # Combined Metrics
        report.append("-" * 40)
        report.append("ğŸ“ çµ„åˆæŒ‡æ¨™:")
        report.append("-" * 40)
        combined = metrics["combined_metrics"]
        report.append(f"  ESS: {combined['ess']} ({combined['ess_ratio']:.0%} of n)")
        report.append(f"  Mean Min-Distance: {combined['mean_min_distance']:.4f}")
        report.append(f"  Unique Combinations: {combined['unique_combinations']}")
        report.append(f"  Bin Coverage: {combined['occupied_bins']}/{combined['total_possible_bins']} ({combined['bin_coverage']:.1%})")
        report.append("")

        # è­¦å‘Š
        if health["warnings"]:
            report.append("-" * 40)
            report.append("âš ï¸ è­¦å‘Š:")
            report.append("-" * 40)
            for w in health["warnings"]:
                report.append(f"  {w}")
            report.append("")

        # çµè«–
        report.append("=" * 60)
        if health["is_real_diversity"]:
            report.append("âœ… çµè«–: å¤šæ¨£æ€§ç‚ºã€çœŸå¯¦å¤šæ¨£æ€§ã€‘")
            report.append("   Big5, Risk Profile ç­‰æ ¸å¿ƒç¶­åº¦åˆ†å¸ƒè‰¯å¥½")
        else:
            report.append("âŒ çµè«–: å¯èƒ½å­˜åœ¨ã€å‡å¤šæ¨£æ€§ã€‘")
            report.append("   éœ€è¦å¢åŠ  Core traits çš„è®Šç•°")
        report.append("=" * 60)

        return "\n".join(report)


# ========== API Functions ==========

def analyze_persona_diversity(personas: List[Dict[str, Any]],
                              entropy_threshold: float = 0.5) -> Dict[str, Any]:
    """åˆ†æ personas çš„å¤šæ¨£æ€§ï¼ˆä¾› API ä½¿ç”¨ï¼‰"""
    monitor = PPVDiversityMonitor(entropy_threshold=entropy_threshold)
    return monitor.compute_diversity_metrics(personas)


def generate_diversity_report(personas: List[Dict[str, Any]],
                              entropy_threshold: float = 0.5) -> str:
    """ç”Ÿæˆå¤šæ¨£æ€§å ±å‘Šï¼ˆä¾› API ä½¿ç”¨ï¼‰"""
    monitor = PPVDiversityMonitor(entropy_threshold=entropy_threshold)
    return monitor.generate_report(personas)


# ========== CLI Test ==========

if __name__ == "__main__":
    import os

    # è¼‰å…¥çœŸå¯¦çš„ Vietnam personas
    script_dir = os.path.dirname(os.path.abspath(__file__))

    all_personas = []

    # å˜—è©¦è¼‰å…¥ vietnam_personas.json
    vietnam1_path = os.path.join(script_dir, "vietnam_personas.json")
    if os.path.exists(vietnam1_path):
        with open(vietnam1_path, 'r', encoding='utf-8') as f:
            personas1 = json.load(f)
            all_personas.extend(personas1)
            print(f"Loaded {len(personas1)} personas from vietnam_personas.json")

    # å˜—è©¦è¼‰å…¥ vietnam2_personas.json
    vietnam2_path = os.path.join(script_dir, "vietnam2_personas.json")
    if os.path.exists(vietnam2_path):
        with open(vietnam2_path, 'r', encoding='utf-8') as f:
            personas2 = json.load(f)
            all_personas.extend(personas2)
            print(f"Loaded {len(personas2)} personas from vietnam2_personas.json")

    if not all_personas:
        print("No personas found!")
        exit(1)

    print(f"\nTotal: {len(all_personas)} personas")
    print("\n" + "=" * 60)
    print("PPV å¤šå…ƒåŒ–ç›£æ¸¬ç³»çµ±æ¸¬è©¦")
    print("=" * 60 + "\n")

    monitor = PPVDiversityMonitor()

    # ç”Ÿæˆå ±å‘Š
    report = monitor.generate_report(all_personas)
    print(report)
