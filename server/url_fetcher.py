"""
URL å…§å®¹æŠ“å–å·¥å…·
ç”¨æ–¼å¾ç¶²é  URL æŠ“å–å…§å®¹ï¼Œè®“ AI å¯ä»¥åŸºæ–¼çœŸå¯¦ç¶²é å…§å®¹å›ç­”å•é¡Œ
æ”¯æ´å‹•æ…‹ç¶²é ï¼ˆJavaScript è¼‰å…¥ï¼‰å…§å®¹æŠ“å– via Playwright
"""
import re
import requests
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# å˜—è©¦åŒ¯å…¥ Playwrightï¼ˆå¯é¸ä¾è³´ï¼‰
try:
    from playwright.sync_api import sync_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False
    print("âš ï¸ Playwright æœªå®‰è£ï¼Œå°‡ä½¿ç”¨åŸºç¤ HTTP æŠ“å–ï¼ˆå‹•æ…‹ç¶²é å…§å®¹å¯èƒ½ä¸å®Œæ•´ï¼‰")

# URL æ­£å‰‡è¡¨é”å¼
URL_PATTERN = re.compile(
    r'https?://[^\s<>"{}|\\^`\[\]ï¼‰ã€‘ã€ã€\)]+',
    re.IGNORECASE
)

def extract_urls(text: str) -> List[str]:
    """
    å¾æ–‡å­—ä¸­æå–æ‰€æœ‰ URL

    Args:
        text: è¦æœå°‹çš„æ–‡å­—

    Returns:
        URL åˆ—è¡¨
    """
    if not text:
        return []

    urls = URL_PATTERN.findall(text)
    # æ¸…ç† URL å°¾ç«¯å¯èƒ½çš„æ¨™é»ç¬¦è™Ÿ
    cleaned_urls = []
    for url in urls:
        # ç§»é™¤å°¾ç«¯çš„æ¨™é»ç¬¦è™Ÿ
        url = url.rstrip('.,;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ')
        if url and url not in cleaned_urls:
            cleaned_urls.append(url)

    return cleaned_urls


def fetch_url_with_playwright(url: str, max_length: int = 5000, wait_time: int = 3000) -> Dict[str, any]:
    """
    ä½¿ç”¨ Playwright æŠ“å–å‹•æ…‹ç¶²é å…§å®¹ï¼ˆæ”¯æ´ JavaScriptï¼‰

    Args:
        url: è¦æŠ“å–çš„ URL
        max_length: å…§å®¹æœ€å¤§é•·åº¦ï¼ˆå­—å…ƒæ•¸ï¼‰
        wait_time: ç­‰å¾… JavaScript è¼‰å…¥çš„æ™‚é–“ï¼ˆæ¯«ç§’ï¼‰

    Returns:
        åŒ…å«æŠ“å–çµæœçš„å­—å…¸
    """
    result = {
        "url": url,
        "success": False,
        "title": "",
        "content": "",
        "error": None
    }

    if not PLAYWRIGHT_AVAILABLE:
        result["error"] = "Playwright æœªå®‰è£"
        return result

    try:
        with sync_playwright() as p:
            # å•Ÿå‹•ç„¡é ­ç€è¦½å™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                viewport={'width': 1280, 'height': 800},
                locale='vi-VN',  # è¶Šå—èªç’°å¢ƒ
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            )
            page = context.new_page()

            # è¨­å®šè¼ƒé•·çš„ timeout
            page.set_default_timeout(15000)

            # å°èˆªåˆ°é é¢
            print(f"  ğŸŒ [Playwright] æ­£åœ¨è¼‰å…¥: {url}")
            page.goto(url, wait_until='networkidle')

            # é¡å¤–ç­‰å¾… JavaScript æ¸²æŸ“
            page.wait_for_timeout(wait_time)

            # å–å¾—æ¨™é¡Œ
            result["title"] = page.title() or urlparse(url).netloc

            # å–å¾—é é¢å®Œæ•´ HTML
            html_content = page.content()

            # è§£æ HTML
            soup = BeautifulSoup(html_content, 'html.parser')

            # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
            for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
                tag.decompose()

            # å˜—è©¦æ‰¾ä¸»è¦å…§å®¹å€
            main_content = None
            for selector in ['main', 'article', '[role="main"]', '.main-content', '.content', '#content', '.article-body', '.page-content']:
                main_content = soup.select_one(selector)
                if main_content:
                    break

            if not main_content:
                main_content = soup.find('body') or soup

            # æå–æ–‡å­—å…§å®¹
            text_content = main_content.get_text(separator='\n', strip=True)

            # æ¸…ç†å¤šé¤˜ç©ºç™½è¡Œ
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
            cleaned_content = '\n'.join(lines)

            # é™åˆ¶é•·åº¦
            if len(cleaned_content) > max_length:
                cleaned_content = cleaned_content[:max_length] + "\n\n[... å…§å®¹å·²æˆªæ–· ...]"

            result["content"] = cleaned_content
            result["success"] = True

            print(f"  âœ“ [Playwright] æŠ“å–æˆåŠŸï¼Œå…§å®¹é•·åº¦: {len(cleaned_content)} å­—")

            browser.close()

    except Exception as e:
        result["error"] = f"Playwright æŠ“å–å¤±æ•—: {str(e)}"
        print(f"  âœ— [Playwright] éŒ¯èª¤: {e}")

    return result


def fetch_url_content(url: str, max_length: int = 3000) -> Dict[str, any]:
    """
    æŠ“å–å–®ä¸€ URL çš„å…§å®¹

    Args:
        url: è¦æŠ“å–çš„ URL
        max_length: å…§å®¹æœ€å¤§é•·åº¦ï¼ˆå­—å…ƒæ•¸ï¼‰

    Returns:
        åŒ…å«æŠ“å–çµæœçš„å­—å…¸ï¼š
        {
            "url": str,
            "success": bool,
            "title": str,
            "content": str,
            "error": str (if failed)
        }
    """
    result = {
        "url": url,
        "success": False,
        "title": "",
        "content": "",
        "error": None
    }

    try:
        # è¨­å®š headers æ¨¡æ“¬ç€è¦½å™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5,zh-TW;q=0.3',
        }

        # ç™¼é€è«‹æ±‚ - æ¸›å°‘ timeout åˆ° 5 ç§’
        response = requests.get(
            url,
            headers=headers,
            timeout=5,  # å¾ 10 ç§’æ¸›å°‘åˆ° 5 ç§’
            allow_redirects=True
        )
        response.raise_for_status()

        # ç¢ºä¿æ­£ç¢ºè™•ç†ç·¨ç¢¼
        if response.encoding is None:
            response.encoding = 'utf-8'

        # è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # å–å¾—æ¨™é¡Œ
        title_tag = soup.find('title')
        result["title"] = title_tag.get_text(strip=True) if title_tag else urlparse(url).netloc

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
            tag.decompose()

        # å˜—è©¦æ‰¾ä¸»è¦å…§å®¹å€
        main_content = None

        # å„ªå…ˆæ‰¾ main, article æˆ–ç‰¹å®š class
        for selector in ['main', 'article', '[role="main"]', '.main-content', '.content', '#content', '.article-body']:
            main_content = soup.select_one(selector)
            if main_content:
                break

        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œä½¿ç”¨ body
        if not main_content:
            main_content = soup.find('body') or soup

        # æå–æ–‡å­—å…§å®¹
        text_content = main_content.get_text(separator='\n', strip=True)

        # æ¸…ç†å¤šé¤˜ç©ºç™½è¡Œ
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        cleaned_content = '\n'.join(lines)

        # é™åˆ¶é•·åº¦
        if len(cleaned_content) > max_length:
            cleaned_content = cleaned_content[:max_length] + "\n\n[... å…§å®¹å·²æˆªæ–· ...]"

        result["content"] = cleaned_content
        result["success"] = True

    except requests.Timeout:
        result["error"] = "è«‹æ±‚è¶…æ™‚"
    except requests.RequestException as e:
        result["error"] = f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {str(e)}"
    except Exception as e:
        result["error"] = f"æŠ“å–å¤±æ•—: {str(e)}"

    return result


def fetch_multiple_urls(urls: List[str], max_length_per_url: int = 2000, use_playwright: bool = True) -> List[Dict]:
    """
    æŠ“å–å¤šå€‹ URL çš„å…§å®¹

    Args:
        urls: URL åˆ—è¡¨
        max_length_per_url: æ¯å€‹ URL å…§å®¹çš„æœ€å¤§é•·åº¦
        use_playwright: æ˜¯å¦ä½¿ç”¨ Playwrightï¼ˆé è¨­ Trueï¼Œç”¨æ–¼å‹•æ…‹ç¶²é ï¼‰

    Returns:
        æŠ“å–çµæœåˆ—è¡¨
    """
    results = []
    for url in urls[:5]:  # æœ€å¤šæŠ“å– 5 å€‹ URL
        # å„ªå…ˆä½¿ç”¨ Playwright æŠ“å–å‹•æ…‹å…§å®¹
        if use_playwright and PLAYWRIGHT_AVAILABLE:
            result = fetch_url_with_playwright(url, max_length_per_url)
            # å¦‚æœ Playwright å¤±æ•—ï¼Œfallback åˆ°åŸºç¤ HTTP
            if not result['success']:
                print(f"  âš ï¸ Playwright å¤±æ•—ï¼Œå˜—è©¦åŸºç¤ HTTP...")
                result = fetch_url_content(url, max_length_per_url)
        else:
            result = fetch_url_content(url, max_length_per_url)

        results.append(result)
        print(f"  {'âœ“' if result['success'] else 'âœ—'} {url[:60]}...")

    return results


def format_url_content_for_prompt(fetch_results: List[Dict]) -> str:
    """
    å°‡æŠ“å–çš„ç¶²é å…§å®¹æ ¼å¼åŒ–ç‚º AI prompt

    Args:
        fetch_results: fetch_multiple_urls çš„å›å‚³çµæœ

    Returns:
        æ ¼å¼åŒ–çš„æ–‡å­—ï¼Œå¯ç›´æ¥åŠ å…¥ prompt
    """
    if not fetch_results:
        return ""

    successful = [r for r in fetch_results if r['success']]

    if not successful:
        return ""

    content_sections = []

    for result in successful:
        # å¦‚æœå…§å®¹å¤ªå°‘ï¼Œè£œå……ç¶²ç«™æè¿°
        content = result['content']
        url = result['url']

        # é‡å° Cathay è¶Šå—ç¶²ç«™è£œå……æè¿°
        if 'cathay-ins.com.vn' in url.lower():
            supplemental_info = """
[WEBSITE CONTEXT - Cathay Insurance Vietnam (åœ‹æ³°ç”¢éšªè¶Šå—)]

This is the official website of Cathay Insurance Vietnam, a subsidiary of Cathay Financial Holdings (Taiwan).

MAIN SECTIONS typically found on this site:
- Homepage: Shows main insurance products and promotional banners
- Du lá»‹ch (Travel Insurance): Travel insurance products for overseas trips
- Sá»©c khá»e (Health Insurance): Health and medical insurance
- Xe (Auto Insurance): Car and motorcycle insurance
- Tai náº¡n (Accident Insurance): Personal accident coverage

TYPICAL WEBSITE FEATURES:
- Vietnamese language interface
- Online quote and purchase options
- Product comparison tools
- Customer service contact information
- Claims process information

NAVIGATION: The site typically has a main menu at the top with product categories,
a hero banner section, and product cards below.

As you browse, pay attention to:
- How easy/difficult it is to find travel insurance information
- The visual design and user experience
- Whether prices and coverage details are clear
- The Vietnamese language quality and clarity
"""
            content = supplemental_info + "\n\n[ACTUAL PAGE CONTENT EXTRACTED]:\n" + content

        section = f"""
---
ğŸ“Œ Website: {result['title']}
ğŸ”— URL: {result['url']}

{content}
---
"""
        content_sections.append(section)

    return f"""
# ğŸ“– REAL WEBSITE CONTENT FOR YOUR REFERENCE:

The interviewer is asking about specific websites. Below is the ACTUAL content from those websites.
You should base your answers on what you SEE in this content, as if you really browsed these websites.

IMPORTANT INSTRUCTIONS FOR WEBSITE BROWSING TASK:
1. Pretend you are ACTUALLY browsing this website right now on your phone/computer
2. Describe what you see on the screen - the layout, colors, images, text
3. Share your REAL reactions as you navigate - what catches your attention, what confuses you
4. Think out loud about what you're looking for and whether you can find it
5. Comment on the user experience from a Vietnamese consumer's perspective

{"".join(content_sections)}

IMPORTANT: When answering questions about these websites, refer to the ACTUAL content above.
Share your genuine reactions and opinions as a Vietnamese consumer viewing these pages.
Describe your browsing journey step by step, as if you're really doing the task.
"""


def extract_and_fetch_urls(question: str, sub_questions: List[str] = None) -> Tuple[List[str], str]:
    """
    å¾å•é¡Œä¸­æå– URL ä¸¦æŠ“å–å…§å®¹çš„ä¾¿åˆ©å‡½æ•¸

    Args:
        question: ä¸»å•é¡Œ
        sub_questions: å­å•é¡Œåˆ—è¡¨

    Returns:
        (urls, formatted_content) tuple
    """
    # åˆä½µæ‰€æœ‰æ–‡å­—
    all_text = question
    if sub_questions:
        all_text += '\n' + '\n'.join(sub_questions)

    # æå– URL
    urls = extract_urls(all_text)

    if not urls:
        return [], ""

    print(f"ğŸŒ Found {len(urls)} URL(s) in question, fetching content...")

    # æŠ“å–å…§å®¹
    results = fetch_multiple_urls(urls)

    # æ ¼å¼åŒ–ç‚º prompt
    formatted = format_url_content_for_prompt(results)

    return urls, formatted


# æ¸¬è©¦
if __name__ == "__main__":
    # æ¸¬è©¦ URL æå–
    test_text = """
    è«‹åƒè€ƒåœ‹æ³°ç”¢éšªçš„æ—…éŠéšªç¶²é  https://www.cathay-ins.com.tw/cathayins/personal/travel/oversea_single_travel/product/
    ä»¥åŠé€™å€‹é é¢ https://www.cathay-ins.com.tw/cathayins/personal/travel/oversea_single_travel/faq/
    """

    urls = extract_urls(test_text)
    print(f"Found URLs: {urls}")

    # æ¸¬è©¦å…§å®¹æŠ“å–
    if urls:
        results = fetch_multiple_urls(urls)
        formatted = format_url_content_for_prompt(results)
        print("\n" + "="*50)
        print(formatted[:1000])
