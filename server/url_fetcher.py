"""
URL å…§å®¹æŠ“å–å·¥å…·
ç”¨æ–¼å¾ç¶²é  URL æŠ“å–å…§å®¹ï¼Œè®“ AI å¯ä»¥åŸºæ–¼çœŸå¯¦ç¶²é å…§å®¹å›ç­”å•é¡Œ
"""
import re
import requests
from typing import List, Dict, Optional, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urlparse

# URL æ­£å‰‡è¡¨é”å¼
URL_PATTERN = re.compile(
    r'https?://[^\s<>"{}|\\^`\[\]ï¼‰ã€‘ã€ã€\)]+',
    re.IGNORECASE
)

def extract_urls(text: str) -> List[str]:
    """
    å¾æ–‡å­—ä¸­æå–æ‰€æœ‰ URL

    Args:
        text: è¦æœå°‹çš„æ–‡å­—

    Returns:
        URL åˆ—è¡¨
    """
    if not text:
        return []

    urls = URL_PATTERN.findall(text)
    # æ¸…ç† URL å°¾ç«¯å¯èƒ½çš„æ¨™é»ç¬¦è™Ÿ
    cleaned_urls = []
    for url in urls:
        # ç§»é™¤å°¾ç«¯çš„æ¨™é»ç¬¦è™Ÿ
        url = url.rstrip('.,;:!?ã€‚ï¼Œï¼›ï¼šï¼ï¼Ÿ')
        if url and url not in cleaned_urls:
            cleaned_urls.append(url)

    return cleaned_urls


def fetch_url_content(url: str, max_length: int = 3000) -> Dict[str, any]:
    """
    æŠ“å–å–®ä¸€ URL çš„å…§å®¹

    Args:
        url: è¦æŠ“å–çš„ URL
        max_length: å…§å®¹æœ€å¤§é•·åº¦ï¼ˆå­—å…ƒæ•¸ï¼‰

    Returns:
        åŒ…å«æŠ“å–çµæœçš„å­—å…¸ï¼š
        {
            "url": str,
            "success": bool,
            "title": str,
            "content": str,
            "error": str (if failed)
        }
    """
    result = {
        "url": url,
        "success": False,
        "title": "",
        "content": "",
        "error": None
    }

    try:
        # è¨­å®š headers æ¨¡æ“¬ç€è¦½å™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5,zh-TW;q=0.3',
        }

        # ç™¼é€è«‹æ±‚ - æ¸›å°‘ timeout åˆ° 5 ç§’
        response = requests.get(
            url,
            headers=headers,
            timeout=5,  # å¾ 10 ç§’æ¸›å°‘åˆ° 5 ç§’
            allow_redirects=True
        )
        response.raise_for_status()

        # ç¢ºä¿æ­£ç¢ºè™•ç†ç·¨ç¢¼
        if response.encoding is None:
            response.encoding = 'utf-8'

        # è§£æ HTML
        soup = BeautifulSoup(response.text, 'html.parser')

        # å–å¾—æ¨™é¡Œ
        title_tag = soup.find('title')
        result["title"] = title_tag.get_text(strip=True) if title_tag else urlparse(url).netloc

        # ç§»é™¤ä¸éœ€è¦çš„å…ƒç´ 
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'aside', 'iframe', 'noscript']):
            tag.decompose()

        # å˜—è©¦æ‰¾ä¸»è¦å…§å®¹å€
        main_content = None

        # å„ªå…ˆæ‰¾ main, article æˆ–ç‰¹å®š class
        for selector in ['main', 'article', '[role="main"]', '.main-content', '.content', '#content', '.article-body']:
            main_content = soup.select_one(selector)
            if main_content:
                break

        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œä½¿ç”¨ body
        if not main_content:
            main_content = soup.find('body') or soup

        # æå–æ–‡å­—å…§å®¹
        text_content = main_content.get_text(separator='\n', strip=True)

        # æ¸…ç†å¤šé¤˜ç©ºç™½è¡Œ
        lines = [line.strip() for line in text_content.split('\n') if line.strip()]
        cleaned_content = '\n'.join(lines)

        # é™åˆ¶é•·åº¦
        if len(cleaned_content) > max_length:
            cleaned_content = cleaned_content[:max_length] + "\n\n[... å…§å®¹å·²æˆªæ–· ...]"

        result["content"] = cleaned_content
        result["success"] = True

    except requests.Timeout:
        result["error"] = "è«‹æ±‚è¶…æ™‚"
    except requests.RequestException as e:
        result["error"] = f"ç¶²è·¯è«‹æ±‚å¤±æ•—: {str(e)}"
    except Exception as e:
        result["error"] = f"æŠ“å–å¤±æ•—: {str(e)}"

    return result


def fetch_multiple_urls(urls: List[str], max_length_per_url: int = 2000) -> List[Dict]:
    """
    æŠ“å–å¤šå€‹ URL çš„å…§å®¹

    Args:
        urls: URL åˆ—è¡¨
        max_length_per_url: æ¯å€‹ URL å…§å®¹çš„æœ€å¤§é•·åº¦

    Returns:
        æŠ“å–çµæœåˆ—è¡¨
    """
    results = []
    for url in urls[:5]:  # æœ€å¤šæŠ“å– 5 å€‹ URL
        result = fetch_url_content(url, max_length_per_url)
        results.append(result)
        print(f"  {'âœ“' if result['success'] else 'âœ—'} {url[:60]}...")

    return results


def format_url_content_for_prompt(fetch_results: List[Dict]) -> str:
    """
    å°‡æŠ“å–çš„ç¶²é å…§å®¹æ ¼å¼åŒ–ç‚º AI prompt

    Args:
        fetch_results: fetch_multiple_urls çš„å›å‚³çµæœ

    Returns:
        æ ¼å¼åŒ–çš„æ–‡å­—ï¼Œå¯ç›´æ¥åŠ å…¥ prompt
    """
    if not fetch_results:
        return ""

    successful = [r for r in fetch_results if r['success']]

    if not successful:
        return ""

    content_sections = []

    for result in successful:
        section = f"""
---
ğŸ“Œ Website: {result['title']}
ğŸ”— URL: {result['url']}

{result['content']}
---
"""
        content_sections.append(section)

    return f"""
# ğŸ“– REAL WEBSITE CONTENT FOR YOUR REFERENCE:

The interviewer is asking about specific websites. Below is the ACTUAL content from those websites.
You should base your answers on what you SEE in this content, as if you really browsed these websites.

{"".join(content_sections)}

IMPORTANT: When answering questions about these websites, refer to the ACTUAL content above.
Share your genuine reactions and opinions as a Vietnamese consumer viewing these pages.
"""


def extract_and_fetch_urls(question: str, sub_questions: List[str] = None) -> Tuple[List[str], str]:
    """
    å¾å•é¡Œä¸­æå– URL ä¸¦æŠ“å–å…§å®¹çš„ä¾¿åˆ©å‡½æ•¸

    Args:
        question: ä¸»å•é¡Œ
        sub_questions: å­å•é¡Œåˆ—è¡¨

    Returns:
        (urls, formatted_content) tuple
    """
    # åˆä½µæ‰€æœ‰æ–‡å­—
    all_text = question
    if sub_questions:
        all_text += '\n' + '\n'.join(sub_questions)

    # æå– URL
    urls = extract_urls(all_text)

    if not urls:
        return [], ""

    print(f"ğŸŒ Found {len(urls)} URL(s) in question, fetching content...")

    # æŠ“å–å…§å®¹
    results = fetch_multiple_urls(urls)

    # æ ¼å¼åŒ–ç‚º prompt
    formatted = format_url_content_for_prompt(results)

    return urls, formatted


# æ¸¬è©¦
if __name__ == "__main__":
    # æ¸¬è©¦ URL æå–
    test_text = """
    è«‹åƒè€ƒåœ‹æ³°ç”¢éšªçš„æ—…éŠéšªç¶²é  https://www.cathay-ins.com.tw/cathayins/personal/travel/oversea_single_travel/product/
    ä»¥åŠé€™å€‹é é¢ https://www.cathay-ins.com.tw/cathayins/personal/travel/oversea_single_travel/faq/
    """

    urls = extract_urls(test_text)
    print(f"Found URLs: {urls}")

    # æ¸¬è©¦å…§å®¹æŠ“å–
    if urls:
        results = fetch_multiple_urls(urls)
        formatted = format_url_content_for_prompt(results)
        print("\n" + "="*50)
        print(formatted[:1000])
